<!DOCTYPE html>
<html lang="en">




<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1.0" name="viewport">




<title>Implementation</title>
<meta content="" name="description">
<meta content="" name="keywords">




<!-- Favicons -->
<link href="assets/img/favicon.png" rel="icon">
<link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">




<!-- Google Fonts -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,600;1,700&family=Montserrat:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&family=Raleway:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">




<!-- Vendor CSS Files -->
<link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
<link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
<link href="assets/vendor/aos/aos.css" rel="stylesheet">
<link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
<link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
<link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">




<!-- Template Main CSS File -->
<link href="assets/css/main.css" rel="stylesheet">




<!-- =======================================================
* Template Name: Nova
* Updated: Jan 09 2024 with Bootstrap v5.3.2
* Template URL: https://bootstrapmade.com/nova-bootstrap-business-template/
* Author: BootstrapMade.com
* License: https://bootstrapmade.com/license/
======================================================== -->
</head>




<body class="page-portfolio">




<!-- ======= Header ======= -->
<header id="header" class="header d-flex align-items-center fixed-top">
  <div class="container-fluid container-xl d-flex align-items-center justify-content-between">




    <a href="index.html" class="logo d-flex align-items-center">
      <!-- Uncomment the line below if you also wish to use an image logo -->
      <!-- <img src="assets/img/logo.png" alt=""> -->
      <h1 class="d-flex align-items-center">Anima V3</h1>
    </a>




    <i class="mobile-nav-toggle mobile-nav-show bi bi-list"></i>
    <i class="mobile-nav-toggle mobile-nav-hide d-none bi bi-x"></i>




    <nav id="navbar" class="navbar">
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="requirements.html">Requirements</a></li>
        <li class="dropdown"><a href="#"><span>Research</span> <i class="bi bi-chevron-down dropdown-indicator"></i></a>
          <ul>
            <li><a href="related.html">Related Projects Review</a></li>
            <li><a href="technology.html">Technology Review</a></li>
            <li><a href="nuitka.html">Nuitka Compilation Issues</a></li>
            <li><a href="pyinstaller.html">Pyinstaller Compilation Issues</a></li>
            <li><a href="pyoxidizer.html">Successful Complilation with PyOxidizer</a></li>
          </ul>
        </li>
        <li><a href="ui-design.html">UI Design</a></li>
        <li><a href="system-design.html">System Design</a></li>
        <li><a href="implementation.html" class="active">Implementation</a></li>
        <li><a href="testing.html">Testing</a></li>
        <li><a href="evaluation.html">Evaluation</a></li>
        <li class="dropdown"><a href="#"><span>Appendices</span> <i class="bi bi-chevron-down dropdown-indicator"></i></a>
          <ul>
            <li><a href="manuals.html">User Manual and Deployment Manual</a></li>
            <li><a href="legal.html">GDPR and Privacy of Data</a></li>
            <li><a href="blog.html">Development Blog</a></li>
            <li><a href="videos.html">Monthly Progress Updates</a></li>
          </ul>
        </li>
      </ul>
    </nav><!-- .navbar -->








  </div>
</header><!-- End Header -->




<main id="main">




  <!-- ======= Breadcrumbs ======= -->
  <div class="breadcrumbs d-flex align-items-center" style="background-image: url('assets/img/implementation-header.jpg');">
    <div class="container position-relative d-flex flex-column align-items-center">
    
      <h2>Implementation</h2>
      <ol>
        <li><a href="index.html">Home</a></li>
        <li>Implementation</li>
      </ol>
    
    </div>
  </div><!-- End Breadcrumbs -->




  <section id="blog" class="blog">
      <div class="container" data-aos="fade-up">
        <div class="row g-5">
          <div class="col-lg-8" data-aos="fade-up" data-aos-delay="200">
              <article class="blog-details">
                 <div class="content">
                   <h1 style="text-align: center;"><strong>Overall Implementation</strong></h1>
                   <p style="text-align: center;">
                    The following are the technologies used for building Anima V3:<br>
                  <h3 style="text-align: center;"><strong>Backend</strong></h3>
                      <p style="text-align: center;">Coqui TTS(Python-based)</p>
                      <p style="text-align: center;">PyOxidizer</p>
                      <p style="text-align: center;">EasyOCR</p>
                    <h3 style="text-align: center;"><strong>Frontend</strong></h3>
                    <p style="text-align: center;">.NET framework</p>
                  </p>
               </article><!-- End blog post -->
            </div>
        </div>
      </div>

      <div class="container" data-aos="fade-up">
          <div class="row g-5">
            <div class="col-lg-8" data-aos="fade-up" data-aos-delay="200">
                <article class="blog-details">
  
                  <div class="content">
                     <h2 style="text-align: center;"><strong>Brief Summary of Backend Technologies</strong></h2>
                       <h4 style="text-align: center;"><strong>Coqui TTS</strong></h4>
                       <div class="post-img">
                        <img src="assets/img/coqui_logo.png" class="img-fluid" alt="" style="max-width: 300px; max-height: 200px;">
                       </div>
                       <br>
                       <p style="text-align: left;">
                       Coqui TTS is an open-source, Python-based text-to-speech (TTS) engine. It allows the conversion of text into speech with natural-sounding voices. This technology is built on machine learning algorithms and can be trained on various datasets to produce voices in different languages and accents. As an open-source application, Coqui TTS provides us with the flexibility to customise the code for specific needs, like voice cloning, and enables us to offer Anima for free, thereby enhancing accessibility and reaching a wider audience.</p>
                       <br>
                       <br>
                       <h4 style="text-align: center;"><strong>PyOxidizer</strong></h4>
                        <div class="post-img" style="display: flex; justify-content: center; align-items: center;">
                          <img src="assets/img/python_to_exe.png" class="img-fluid" alt="" style="max-width: 300px; max-height: 200px;">
                        </div>
                       <br>
                       <p style="text-align: left;">
                       PyOxidizer is a tool used to package Python applications into standalone executables. Given Python's prominence in AI, including text-to-speech technologies, many high-quality, open-source TTS engines are written in Python. For our project, Anima, we needed to convert our Python-based application into a Windows desktop executable to meet our deployment requirements. We evaluated both Nuitka and PyOxidizer for this purpose. Despite initial attempts with Nuitka, we encountered several issues and ultimately found success with PyOxidizer, which proved to be more reliable for packaging our Python code into an executable format suitable for a Windows environment.</p>
                       <br>
                       <br>
                       <h4 style="text-align: center;"><strong>EasyOCR</strong></h4>
                       <div class="post-img">
                        <img src="assets/img/easyocr.png" class="img-fluid" alt="" style="max-width: 300px; max-height: 200px;">
                       </div>
                       <br>
                       <p style="text-align: left;">
                       EasyOCR is a tool designed for optical character recognition (OCR), capable of converting images of text into editable and searchable text. It's known for its simplicity and support for multiple languages, making it a versatile choice for OCR needs. We incorporated EasyOCR into our app, Anima, to enhance its ease of use and versatility. This addition also means users donâ€™t have to manually copy text from PDFs to read a book; instead, they can simply upload the PDF, and EasyOCR facilitates the text extraction. By integrating OCR technology, we aimed to streamline the user experience.</p>
                       <br>
                       <br>
                       <h4 style="text-align: center;"><strong>.NET</strong></h4>
                       <div class="post-img">
                        <img src="assets/img/net_logo.png" class="img-fluid" alt="" style="max-width: 300px; max-height: 200px;">
                       </div>
                       <br>
                       <p style="text-align: left;">
                       The .NET framework is a comprehensive development platform used for building and running applications on Windows. Key advantages of the .NET framework over the Microsoft Foundation Classes (MFC) include improved development speed due to its extensive library of pre-coded solutions and .NET offers easier integration with other Microsoft technologies, making it a more versatile and modern choice for developing Windows applications. Read more about our research about .NET vs MFC <a href="technology.html">here</a>.</p>
                  
                     </p>
                </article><!-- End blog post -->
              </div>
          </div>
        </div>

      <div class="container" data-aos="fade-up">
       <div class="row g-5">
         <div class="col-lg-8" data-aos="fade-up" data-aos-delay="200">
             <article class="blog-details">
               <div class="content">
                <h2 style="text-align: center;"><strong>The Structure of the Explanation for Implementation</strong></h2>
                 <p style="text-align: center;">
                   To best explain how we implemented Anima, we will use the following approach for each <strong>key feature</strong>:
                       <p style="font-weight: 700; font-style: italic; text-align: center;">a) Explain the the design and implementation of the .NET frontend</p>
                       <p style="font-weight: 700; font-style: italic; text-align: center;">b) Describe the JSON communication between Python backend and the .NET frontend</p>
                       <p style="font-weight: 700; font-style: italic; text-align: center;">c) Explain the principle and implementation of the Python backend</p>
    
                  
                   <img src="assets/img/flow_exp.png" alt="Description of the image">


                  </p>
             </article><!-- End blog post -->
           </div>
       </div>
     </div>

     <div class="container" data-aos="fade-up">
      <div class="row g-5">
        <div class="col-lg-8" data-aos="fade-up" data-aos-delay="200">
            <article class="blog-details">
              <div class="content">
               <h2 style="text-align: center;"><strong>The Frontend Design</strong></h2>
               <h3 style="text-align: left;">First Time User and Existing User Pathways</h3>
                <p style="text-align: left;">
                  The frontend of Anima was designed with ease of use, intuitiveness, and accessibility in mind. The user interface was created using the .NET framework, which is known for its user-friendly design and compatibility with Windows applications. 
                  The frontend design includes two pathways for users: First time user and existing user pathways.</p>
                  <h3 style="text-align: center;">First Time User</h3>
                  <div>
                    <img src="assets/img/ui-design/final/home.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 400px;">
                    <img src="assets/img/arrow-right.svg" alt="">
                    <img src="assets/img/ui-design/final/read.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 400px;">
                  </div><br>
                                  

                  <h3 style="text-align: center;">Existing User</h3>
                  <div>
                    <img src="assets/img/ui-design/final/existing_user.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 400px;">
                    <img src="assets/img/arrow-right.svg" alt="">
                    <img src="assets/img/ui-design/final/tts.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 400px;">
                  </div><br>

                <h3 style="text-align: left;">The 3 Window Design</h3>
                <p style="text-align: left;">
                  Anima V3 has been designed with 3 main windows in mind. These windows are the Text-to-Speech Window, Bank Voice Window, and Manage Voicebank Window. To make the 3 window approach as clear and user-friendly as possible, we have designed the windows to be easily navigable through the window navigation buttons at the top right corner of each window.
                </p>
                  <div class="post-img">
                    <img src="assets/img/three_window.png" class="img-fluid" alt="" style="max-width: 800px; max-height: 550px;">
                  </div>
                  <h4 style="text-align: center;">Text-to-Speech Window</h4>
                  <p style="text-align: left;">
                  The Text-to-Speech Window is the main window where the user can input text and speak with the digital voice that they have created.
                  Text-to-Speech window also includes the option to read from a PDF or image (jpeg/png) file.  
                  </p>
                  <div class="post-img">
                    <img src="assets/img/ui-design/final/tts.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 400px;">
                  </div><br>
                
                  <h4 style="text-align: center;">Bank Voice Window</h4>
                  <p style="text-align: left;">
                  The Bank Voice Window is where the user can record their voice and save it to the voicebank. In this window, the user can also choose the language of the voice they want to record. The voicebanking text changes according to the language selected and the model becomes 
                  biased towards the intonations of the language.</p>
                  <div class="post-img">
                    <img src="assets/img/ui-design/final/read.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 400px;">
                  </div><br>

                  <h4 style="text-align: center;">Manage Voicebank Window</h4>
                  <p style="text-align: left;">
                   The Manage Voicebank Window is where the user can manage their voicebank. This includes the ability to import a voice in the user's selected language, deleting a voice profile and selecting another voice profile to be their digital voice.</p>
                  <div class="post-img">
                    <img src="assets/img/ui-design/final/manage.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 400px;">
                  </div><br>
                </p>

            </article><!-- End blog post -->
          </div>
      </div>
    </div>

    <div class="container" data-aos="fade-up">
      <div class="row g-5">
        <div class="col-lg-8" data-aos="fade-up" data-aos-delay="200">
            <article class="blog-details">
              <div class="content">
               <h2 style="text-align: center;"><strong>The Implementation of the Frontend</strong></h2>
                <h3 style="text-align: left;">The Utilization of .NET Framework</h3>
                <p style="text-align: left;">The .NET framework works through the combination of the C# programming language and the XAML markup language. In this sense it is very similar to JavaScript and HTML, however it is much easier to use. XAML is extremely easy to use, from the toolbox you can drag and drop the button, textboxes, images and more and move them around the window by dragging them around. This makes designing the contents of a window very user-friendly and efficient.</p>
                <div class="post-img">
                  <img src="assets/img/toolbox.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 500px;">
                </div>
                <p style="text-align: left;">Moreover, tools come with many properties that can be edited with C#. Via the 'Name' property, each tool is identified and then anything about the tool can be changed with C# such as its visibility, content, position and more. It is also possible to define the functionality of a tool by attaching a function to the 'Click' property of a tool (for Button tools, for TextBox, it is the TextChanged property). Through the extensive use of the explained methods, the frontend of Anima V3 was designed and the functionalities were implemented.</p>
                <div>
                  <img src="assets/img/xamlex.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 400px;">
                  <img src="assets/img/csharpex.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 400px;">
                  <img src="assets/img/csharpnameex.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 400px;">
                </div>
                <h3 style="text-align: left;">The Implementation of the Main Functionalities</h3>
                <p style="text-align: left;">The main functionalities of Anima V3 include banking your voice to register an animprofile and creating a digital voice (voice cloning), speaking with your digital voice (registered animaprofile), reading from a file using your digital voice, and importing a .wav file to register an animaprofile.</p>
                <h4 style="text-align: center;">Creating a Digital Voice - Voice Cloning</h4>
                <p style="text-align: left;">The users bank their voice by reading through a 10 page voice-banking text (10 + 1 clicks as the last click is for stopping the recording). The recording is done in the frontend, with the help of a .NET library, NAudio. When the users finish reading the text, their voice is stored in a output.wav file, ready for processing by the Anima API to be converted into an .animaprofile file. The actual processing happens on the backend, the frontend simply writes to the frontend.json, which is an instruction picked up by the backend. </p>
                <div>
                  <img src="assets/img/profile_registeration.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 600px;">
                  <img src="assets/img/play_end_recording.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 600px;">
                </div>
                <p style="text-align: left;">Here, it's also possible to see one of the failsafe's implemented for keeping the application from crashing.
                The voice creation button is disabled after the frontend.json is changed and the instruction has been sent to the backend. In this case, since we are counting the clicks to send instructions to backend, the application wouldn't crash, but would give the user the wrong idea about the success of the voice cloning. This also increases the maintainability and improvability of the codebase as the click counting architecture can be changed and the code would continue to work without problems.</p><br>
                
                <h4 style="text-align: center;">Speaking with your Digital Voice</h4>
                <p style="text-align: left;">The user can speak with their digital voice by typing in the text box in the Text-to-Speech window and clicking the speak button. The content of the text box is then sent to the backend, where it is processed by the Anima API to speak with the digital voice of the user.
                </p>
                <div class="post-img">
                  <img src="assets/img/speak.png" class="img-fluid" alt="" style="max-width: 600px; max-height: 600px;">
                </div>
                <p style="text-align: left;">Again, there is multiple precautions here to make sure that the application is working as intended. Since the backend recognises instructions through the changes in the frontend.json file, giving the same content multiple times is not natively recognized as there will be no change in the JSON file. Hence, we have introduced UUID's (unique identifiers) to recognizes that there is another speak attempt by the user even if the content is the same. We have also ensured that the backend has stopped processing the instruction through asynchronous functions that check the success messages. Lastly, this function also includes a failsafe for the user which is the initial 'if statement' in the code. If the user has already clicked speak and the computer is speaking with the user's digital voice, the user can hit the same button again to stop the speech.</p><br>

                <h4 style="text-align: center;">Reading from a File (PDF, PNG, JPG) Using OCR</h4>
                <p style="text-align: left;">In the frontend, the read from image functionality works with the same principle as the other functionalities. The file path and the UUID of the file is written to frontend.json and an asynchronous function awaits the response of the backend. For this functionality the Windows API is used selecting a file. Through a dialog box, which is basically the file explorer in Windows, the user is allowed to choose any file they want.</p>

                <div class="post-img">
                  <img src="assets/img/read_from_image.png" class="img-fluid" alt="" style="max-width: 900px; max-height: 600px;">
                </div>

                <p style="text-align: left;">If the speak functionality fails, there is two possible reasons, either the file is of the right format (PNG, JPG, PDF) and the quality is not good enough for backend figure out the text on the file, or the file format is incorrect. We let the user know through a message box via the Windows API.</p>

                <h4 style="text-align: center;">Importing a Voice to Create a Digital Voice</h4>

                <p style="text-align: left;">The import voice functionality is in the Manage Voicebank window and is almost identical to read from image in terms of the Windows dialogue and the error we get when a file of type different than .wav is selected. The only difference is when the import is successful, there needs to be the created and usable voice profile on the Manage Voicebank window.</p>

                <div class="post-img">
                  <img src="assets/img/import_voice.png" class="img-fluid" alt="" style="max-width: 900px; max-height: 600px;">
                </div>

                <p style="text-align: left;">In the code, to achieve this, we are removing all the radio buttons displayed in the Manage Voicebank window (except the selected voice because it's component is permanent in the XAML and only changes visibility - never removed or added) and then updating the window with all the radio buttons after the successful voice profile creation with the imported voice.</p>


              
            </article><!-- End blog post -->
          </div>
      </div>
    </div>

    <div class="container" data-aos="fade-up">
      <div class="row g-5">
        <div class="col-lg-8" data-aos="fade-up" data-aos-delay="200">
            <article class="blog-details">
              <div class="content">
               <h2 style="text-align: center;"><strong>The Implementation of Communication between Frontend and Backend</strong></h2>
               <h3 sytle="text-align: left;">How does communication between frontend and backend work?</h3>
               <p style="text-align: left;">The idea behind the JSON communication is incredibly simple. The frontend write to a JSON file, frontend.JSON.The backend has an observer class that observes this file and detects the changes. The JSON key of the value that has changed correspond to a certain action of tha backend API. When that action has been performed, the backend decides whether it has been a success or failure. It then writes its feedback on a JSON file, backend.JSON. This file is read by the frontend to let it know what to display to the user.</p>
                <h3 style="text-align: left;">The Structure of the JSON Files</h3>
               <div>
                <img src="assets/img/frontendJSON.png" class="img-fluid" alt="" style="max-width: 800px; max-height: 600px;">
                <img src="assets/img/backendJSON.png" class="img-fluid" alt="" style="max-width: 800px; max-height: 600px;">
               </div>
              
               <p style="text-align: left;">Above are the two JSON files mentioned. This is the state of the JSON files before the application is used. The JSON keys change according to the actions the user takes in the frontend.</p>
              
               <h3 style="text-align: left;">Basic Communication Example</h3>
               <div>
                <img src="assets/img/frontendJSONafter.png" class="img-fluid" alt="" style="max-width: 800px; max-height: 600px;">
                <img src="assets/img/backendJSONafter.png" class="img-fluid" alt="" style="max-width: 800px; max-height: 600px;">
               </div>

                <p style="text-align: left;">Above are the two JSON files after the application is used. The application was used to create a voice profile with the name 'Uras' and then speak the words 'Hello, how are you?'. In the frontend.JSON, profile creation is done through the change in the 'speakerName' key in the JSON file. When a new profile is created, the 'nameOfCurrentUser' key is automatically updated to be the newly created profile. Because the profile creation was successful, the 'profileCreationSuccess' key in the backend.JSON is set to true. This key has not reset it's value here because of before the instruction of creating a new profile is sent to the backend, this value is automatically set to false.<br>
                
                Computer speaking with the user's voice facilitated through the change in the 'content' key of frontend.json. The 'speakID' is to make sure that the instruction is recognized by the backend, even if the content has stayed the same. Again, the 'speakSuccess' key in the backend.json is set to true because the instruction was successful. Just like 'profileCreationSuccess', it will be reset before the next instruction of speaking with the digital voice.</p>
            </article><!-- End blog post -->
          </div>
      </div>
    </div>

    <div class="container" data-aos="fade-up">
      <div class="row g-5">
        <div class="col-lg-8" data-aos="fade-up" data-aos-delay="200">
            <article class="blog-details">
              <div class="content">
               <h2 style="text-align: center;"><strong>The Implementation of the Backend</strong></h2>
               <p style="text-align: left;">
                The backend of Anima V3 has been built upon the idea of responding to the changes in frontend.json and performing the relevant action using
                using the Anima API. The Anima API is an encapsulation of the CoquiTTS functions that are being used to produce sounds, or registering a profile.
                </p><br>

                <h3 style="text-align: left;">The Observer Class</h3>
                <p style="text-align: left;">
                First and most importantly, the backend detects changes through an observer class. Here is the implementation:
               </p>

              <div class="post-img">
                <img src="assets/img/observer.png" class="img-fluid" alt="" style="max-width: 800px; max-height: 600px;">
              </div>

              <p style="text-align: left;">
                The observer class works via the detectChanges method. In this method, it keeps reloading the frontend.json and checks whether there has been a change in the file by comparing against the not-reloaded verison of the json file. If there has been a change, then it calls the determineChanges method to figure out what has changed, and returns the changed key-value pairings.
              </p><br>
            
              <h3 style="text-align: left;">Main loop</h3>
              <p style="text-align: left;">
              It is now obvious how the changes are detected in the frontend.json file. However, how we constantly check the frontend.json file is still not clear. This is done through within the entry point of the Python backend. The enrtry point is the main function that contains a while True loop that checks the frontend.json file every 0.5 seconds (detect period). 
              </p>

              <div>
                <img src="assets/img/mainloop1.png" class="img-fluid" alt="main_loop_start" style="max-width: 600px; max-height: 500px;">
                <img src="assets/img/mainloop3.png" class="img-fluid" alt="main_loop_end" style="max-width: 600px; max-height: 500px;">
              </div>
              <p style="text-align: left;">
              In the while True loop, the code checks which keys have been subjected to change through the button clicks in the frontend. According to the changes, the backend performs a different action as seen in the above code. Also notice that the main loop is wrapped in a try-except block. In this case, the only errors not caught are the KeyboardInterrupt or the closing the application. This means that the application runs indefinitely until the user decides to close it. Now, we will see how the backend works with an example. Consider the case where in the frontend the user has written some text and pressed the SPEAK button to speak with their unique digital voice. The frontend.json has been changed accordingly, how does the backend work to make the computer speak?
              </p><br>

              <h3 style="text-align: left;">The SPEAK example</h3>
              <p style="text-align: left;">The speak functionality depends on the frontend.json keys "speakID" and "content". It depends on two keys becuase of one single fact: The user might want to say the same sentence twice. In this case, the "content" of their speech wouldn't change therefore there wouldn't be anything for our observer class to pick-up. We ensure everytime the user hits the SPEAK button is picked up through the instruction of UUID's, in this case "speakerID". UUID is Universially Unique Identifier, which is an always unique 128 bit identifier that helps backend pick-up each instruction from the frontend.</p>
              <div class="post-img">
                <img src="assets/img/speakfunc.png" class="img-fluid" alt="speak_example" style="max-width: 800px; max-height: 600px;">
              </div>

              <p style="text-align: left;">
              In the above code, we initially check if "speakID" is within changes, if yes, this means that the user has pressed the SPEAK button and is trying to speak with their digital voice. We then check whether the "content" key has been changed. If it has been changed, we want to make the computer speak with new content, and if not, we make the computer speak with the same content as the last time. We then call the computerSpeak function which is a function that encapsulates the Anima API for creating speech.
              </p><br>
              <h3 style="text-align: left;">The computerSpeak function</h3>
              <div class=post-img>
                <img src="assets/img/computerspeak.png" class="img-fluid" alt="computerspeak" style="max-width: 800px; max-height: 600px;">
              </div>

              <p style="text-align: left;">
                In the computerSpeak method, we parse the content of the speech which basically getting rid of extra space characters, send the parsed content to Anima API to be processed with the digital voice created for the user and return the waveforms created of the audio. Then we play the waveforms for the user, which is the speech that the user created with their unique digital voice.
              </p><br>

              <h3 style="text-align: left;">The Anima API - Speak Functionality</h3>
              
              <div class=post-img>
                <img src="assets/img/wavfromprofile.png" class="img-fluid" alt="wav_from_profile" style="max-width: 800px; max-height: 600px;">
              </div>
              <p style="text-align: left;">
                Anima API includes functions that are encapsulations of the CoquiTTS functions for specific use with the voicebanking solution. The wav_from_profile method is one of these functions that takes the content of the speech and the profile of the user and returns the waveforms of the speech. The waveforms are then played to the user through the computerSpeak method. However, we have created a custom synthesizer from the synthesizer class in the CoquiTTS library to make the speech sound more natural. So, the last step of the backend is to call the ttsFromProfile method of the custom synthesizer.
              </p><br>

              <h3 style="text-align: left;">The Custom Synthesizer and ttsFromProfile</h3>
              
              <p style="text-align: left;">
                The last stage of the SPEAK functionality is the ttsFromProfile method of the custom synthesizer. This method utilizes the synthesisFromProfile method, which is basically a CoquiTTS method that synthesize the speech from the profile of the user. The CoquiTTS function works by loading the pre-trained TTS model (registered profile) and converting the text into a sequence of token IDs and then into a Torch tensor. Then the text tensor is passed through the neural network model which is the .animaprofile file of the user.   
              </p>

              <div class=post-img>
                <img src="assets/img/ttsfromprofile.png" class="img-fluid" alt="ttsFromProfile" style="max-width: 800px; max-height: 600px;">
              </div>
              <p style="text-align: left;">
                The ttsFromProfile method functions by taking the content of the speech and the profile of the user and splitting the content of the speech into sentences. For each sentence in the content of the speech, we get the output waveforms returned by the synthesisFromProfile method. We process the synthesized output including denormalization, normalization and resampling based on vocoder and TTS audio configurations selected. We have also chose to enable trim silence in the TTS config and decided that the audio the computer is better with this setting on. We havenâ€™t touched the CoquiTTS code and therefore after this step, we start returning the waveforms, until it can be played at the backend entry point by the userâ€™s computer.
              </p><br>

              <h3 style="text-align: left;">Feedback from Backend</h3>
              <p>
                The backend writes to the backend.json file after each instruction has been processed. The frontend reads the backend.json file to know whether the instruction has been successful or not. The following are the images of the main loop of the backend entry point writing to the backend.json and the frontend reading from the backend.json.
              </p>
                <div>
                  <img src="assets/img/writingtobackendjson.png" class="img-fluid" alt="backendJSONafter" style="max-width: 800px; max-height: 600px;">
                  <img src="assets/img/speakfrontend.png" class="img-fluid" alt="backendJSONafter" style="max-width: 800px; max-height: 600px;">
                </div>
              
                <p style="text-align: left;">
                  In the above left image, the speak functionalitiy code of the backend is seen. After the backend performs it's operation, the "speechSuccess" message is written to the backend.json file as a success message. And in the above right image, the frontend code that processes the success message is seen. The frontend waits until the success message is written to the backend.json file using the asynchronous WaitSpeech method. Once backend writes to backend.json, speech is either a success or not. If it's a success, there is no need to notify the user since they will be hearing their digital voice through the computer. However, if the speech has failed, we let ther user know through a message box pop-up.</p>

            </article><!-- End blog post -->
          </div>
      </div>
    </div>

    <div class="container" data-aos="fade-up">
      <div class="row g-5">
        <div class="col-lg-8" data-aos="fade-up" data-aos-delay="200">
            <article class="blog-details">
              <div class="content">
                <h2 style="text-align: center;">Generalization - Does all backend functionality work this way?</h2>
                <p style="text-align: left;">
                  All the main functionalities of Anima V3 work with same/similar steps.
                </p>
                  <ul>
                    <li>Frontend writes to frontend.json</li>
                    <li>Backend detects changes in frontend.json through the observer class</li>
                    <li>Backend performs the relevant action with the Anima API</li>
                    <li>Backend writes to backend.json</li>
                    <li>Frontend reads from backend.json</li>
                  </ul> 
                <p style="text-align: left;">
                  Each step encapsulates the actions of CoquiTTS, the engine that does the voice-cloning and speech. With these encapsulations we were able to produce an application that adheres important software development principles such as the Single Responsibility Principle, Open/Closed Principle and the Abstraction Principle.
                  <br>
                  Read more about our software design principles <a href="system-design.html">here</a>.
                </p>
              </div>
            </article><!-- End blog post -->
          </div>
      </div>
    </div>
       
        </section><!-- End Portfolio Section -->
      <!-- ======= Portfolio Section ======= -->
     </main><!-- End #main -->
    <!-- ======= Footer ======= -->
  <footer id="footer" class="footer">

    <div class="footer-content">
      <div class="container">
        <div class="row gy-4">
          <div class="col-lg-5 col-md-12 footer-info">
            <a href="index.html" class="logo d-flex align-items-center">
              <span>Anima V3</span>
            </a>
            <p>A software that preserves the person's voice so that even when their natural voice is lost, their sense of self is not.</p>
            <br>
            <p>This is our project website for Anima V3.</p>
            <br>
          </div>
        </div>
      </div>
    </div>

    <div class="footer-content">
      <div class="row" data-aos="zoom-in">
        <div class="d-flex justify-content-between align-items-center w-10">
          <p><strong>Our Project Partners: </strong></p>
          <a href="https://www.ucl.ac.uk/"><img src="assets/img/ucl.png" class="larger-image" alt=""></a>
          <a href="https://www.gosh.nhs.uk/"><img src="assets/img/gosh.png" class="larger-image" alt=""></a>
          <a href="https://www.als-mnd.org/"><img src="assets/img/als.png" class="larger-image" alt=""></a>

          <p><strong>With Support From: </strong></p>
          <a href="https://www.intel.com/" title="With Support From"><img src="assets/img/intel.png" class="larger-image" alt=""></a>
        </div>
      </div>
    </div>

    <div class="footer-legal">
      <div class="container">
        <p>Bootstrap template details:</p>
        <div class="copyright">
          &copy; Copyright <strong><span>Nova</span></strong>. All Rights Reserved
        </div>
        <div class="credits">
          <!-- All the links in the footer should remain intact. -->
          <!-- You can delete the links only if you purchased the pro version. -->
          <!-- Licensing information: https://bootstrapmade.com/license/ -->
          <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/nova-bootstrap-business-template/ -->
          Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
        </div>
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
